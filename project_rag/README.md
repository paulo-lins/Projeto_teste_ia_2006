# ðŸ¤– RAG Hands-On: Arquitetura e OtimizaÃ§Ã£o de Busca SemÃ¢ntica

Este repositÃ³rio foi desenvolvido para fins educacionais, servindo como um guia prÃ¡tico para estudantes que desejam entender o funcionamento de sistemas **RAG (Retrieval-Augmented Generation)**. 

O projeto demonstra como transformar documentos estÃ¡ticos (.txt) em uma base de conhecimento dinÃ¢mica para LLMs (Modelos de Linguagem de Grande Escala), permitindo que a IA responda perguntas baseada em dados locais e privados.

---

## ðŸŽ¯ VisÃ£o Geral do Projeto

A tÃ©cnica de RAG resolve um dos maiores problemas das IAs atuais: a "alucinaÃ§Ã£o" e o desconhecimento de dados recentes ou privados. Aqui, implementamos um pipeline completo que "ensina" a IA a consultar arquivos antes de formular uma resposta.

---

## ðŸ“‚ EvoluÃ§Ã£o e Diferenciais das VersÃµes

O projeto estÃ¡ estruturado em duas etapas que mostram a evoluÃ§Ã£o de um desenvolvedor:

### 1. VersÃ£o 1: ImplementaÃ§Ã£o Base (`estudo_ia_v1.py`)
* **Objetivo:** Validar o fluxo de ponta a ponta (IngestÃ£o -> Embedding -> Busca -> Resposta).
* **CaracterÃ­sticas:** ComentÃ¡rios em inglÃªs para familiarizaÃ§Ã£o com o vocabulÃ¡rio tÃ©cnico global.
* **LimitaÃ§Ã£o:** Processa todos os documentos toda vez que Ã© executado, gerando redundÃ¢ncia.

### 2. VersÃ£o 2: OtimizaÃ§Ã£o e Performance (`estudo_ia_v2.py`)
* **Melhoria TÃ©cnica:** ImplementaÃ§Ã£o de lÃ³gica de **IdempotÃªncia**.
* **O que mudou:** O script agora utiliza uma verificaÃ§Ã£o de existÃªncia (`any()`) para checar se cada arquivo jÃ¡ possui "chunks" (pedaÃ§os) vetorizados no banco local (`vector_db`).
* **Valor:** Reduz drasticamente o uso de CPU/GPU e o tempo de execuÃ§Ã£o, simulando um ambiente de produÃ§Ã£o real onde performance Ã© custo.

---

## ðŸ› ï¸ Stack TecnolÃ³gica e DecisÃµes de Projeto

| Ferramenta | Papel no Sistema | Justificativa |
| :--- | :--- | :--- |
| **Python 3.13** | Linguagem Base | LÃ­der em ecossistemas de IA e processamento de dados. |
| **Sentence-Transformers** | Embeddings | Modelo `all-MiniLM-L6-v2`: leve (roda em CPU) e eficiente para parÃ¡grafos. |
| **OpenAI SDK** | Interface Universal | Usado como "ponte" para a Groq. Seguir este padrÃ£o permite trocar de provedor (OpenAI, Ollama, Anthropic) mudando apenas o `base_url`. |
| **Groq (Llama 3.3)** | Motor de InferÃªncia | Provedor que oferece velocidade extrema (LPUs) e modelos Open Source de alta performance. |
| **Pickle (.pkl)** | Banco de Dados Local | PersistÃªncia binÃ¡ria dos vetores, facilitando o estudo sem a complexidade de um banco externo. |

> **Nota TÃ©cnica:** Embora o cÃ³digo utilize `import openai`, estamos conectando Ã  API da **Groq**. Fizemos isso para seguir o padrÃ£o de mercado (OpenAI-compatible API), o que torna o cÃ³digo flexÃ­vel para futuros provedores.

---

## ðŸš€ Como Configurar e Rodar

### 1. Estrutura de Pastas
```text
project_rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ estudo_ia_v1.py
â”‚   â””â”€â”€ estudo_ia_v2.py
â”œâ”€â”€ my_documents/
â”‚   â”œâ”€â”€ files_txt/        # Coloque seus arquivos .txt aqui
â”‚   â””â”€â”€ vector_db/         # Gerado automaticamente (.pkl)
â”œâ”€â”€ .env                  # Chave de API (GROQ_API_KEY)
â”œâ”€â”€ .gitignore            # Ignora .env, .pkl e venv
â””â”€â”€ requirements.txt